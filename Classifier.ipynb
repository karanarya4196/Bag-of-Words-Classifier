{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:17.336294Z",
     "start_time": "2019-09-20T10:19:54.852796Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karan.Arya\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (26,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from Spacy_Tagging import get_pos_entities_parsed, expandText, expandTextNew\n",
    "from Wordnet_functionality import get_rootword, isWordPresentInWordnet\n",
    "# from Task_Extraction_From_LBA_text import get_task_list, is_action_task\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "code = ['A103']\n",
    "folder_to_save = 'A103/'\n",
    "training_set = pd.read_csv('Swiss Re-Training_' + code[0] + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating features for sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:17.559328Z",
     "start_time": "2019-09-20T10:20:17.554296Z"
    }
   },
   "outputs": [],
   "source": [
    "def getTextFromPOSData(data):\n",
    "    return ' '.join([d.split('/')[0] for d in data.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:17.793289Z",
     "start_time": "2019-09-20T10:20:17.783285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.588"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_set) * .0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:18.016284Z",
     "start_time": "2019-09-20T10:20:18.008285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Threshold:  23\n"
     ]
    }
   ],
   "source": [
    "threshold_for_vocab_selection = int(len(training_set) * .0005)\n",
    "print('Selected Threshold: ', threshold_for_vocab_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:18.264277Z",
     "start_time": "2019-09-20T10:20:18.257280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Carrier', 'Company Name', 'Coverage Counsel Guidelines',\n",
       "       'Invoice Date', 'Invoice End Date', 'Invoice Line Item Id',\n",
       "       'Invoice No.', 'Invoice Start Date', 'Invoice Status',\n",
       "       'Invoice Status Date', 'Invoice System ID No',\n",
       "       'Line Item Adjustment Description', 'Line Item Adjustment Reason',\n",
       "       'Line Item Amount', 'Line Item Client Adjustment',\n",
       "       'Line Item Date of Service', 'Line Item Description',\n",
       "       'Line Item ITP Adjustment', 'Line Item Number', 'Line Item Rate',\n",
       "       'Line Item Reviewer taking Adjustment',\n",
       "       'Line Item TPA Appeal Adjustment', 'Line Item TPA Net Adjustment',\n",
       "       'Line Item TPA Reviewer Adjustment', 'Line Item Units',\n",
       "       'Line Item Vendor Adjustment', 'ML Adj Desc', 'ML Adj Level',\n",
       "       'ML Adj Reason', 'ML Output 1', 'ML Output 2', 'ML Output 3',\n",
       "       'ML Score', 'Matter Id', 'Matter No.', 'Received Date', 'Timekeeper',\n",
       "       'Timekeeper Level', 'UTBMS Activity Code', 'UTBMS Activity Description',\n",
       "       'UTBMS Phase Code', 'UTBMS Phase Description', 'UTBMS Task Code',\n",
       "       'UTBMS Task Description', 'Work Area', 'Work Area Level 2', '_merge',\n",
       "       'Adjusted', 'pos', 'entities', 'parsed', 'Is_Prior_Approved',\n",
       "       'Approver_Name', 'Task_list', 'Is_Action'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:18.498273Z",
     "start_time": "2019-09-20T10:20:18.495277Z"
    }
   },
   "outputs": [],
   "source": [
    "# training_set['Line Item Description'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:18.726266Z",
     "start_time": "2019-09-20T10:20:18.722272Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_word_list = ['void', 'each', 'weigh', 'clip', 'spanish', 'a.m.', 'become', 'semi', 'snow', 'mean', 'cash', 'known', 'many', 'boyfriend', 'example', 'wife', 'ways', '12th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:39.506825Z",
     "start_time": "2019-09-20T10:20:18.960264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4463\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {}\n",
    "for line in training_set[['pos', 'entities']].itertuples():\n",
    "    pos    = line[1];  pos_modified = pos\n",
    "    entities = line[2]\n",
    "    \n",
    "    if entities!=None and type(entities)!=float:\n",
    "        if len(entities.strip()) ==0:\n",
    "            continue\n",
    "        for entity in entities.split('\\t'):\n",
    "            #print(entity)\n",
    "            entity_split = entity.split('/')\n",
    "            entity_text  = entity_split[0]\n",
    "            tag          = entity_split[1]\n",
    "            for entity_text_split in entity_text.split(' '):\n",
    "                if len(entity_text_split.strip())>0:\n",
    "                    pos_modified = pos_modified.replace(entity_text_split, '<' + tag + '>')\n",
    "    \n",
    "    for word in pos_modified.split(' '):\n",
    "        word_split = word.split('/')\n",
    "        word_lower = word_split[0].lower()\n",
    "        if isWordPresentInWordnet(word_lower) or re.search(r'<[A-Za-z]+>', word_lower):\n",
    "            try:\n",
    "                word_lower = get_rootword(word_lower, word_split[1])\n",
    "            except:\n",
    "                pass\n",
    "                #print(pos)\n",
    "                #print()\n",
    "                #print(entities)\n",
    "                #print()\n",
    "                #print(pos_modified)\n",
    "            #print(word_lower)\n",
    "            if len(word_lower)>=4 and len(word_split)>1 and word_split[1] in ['VERB', 'ADJ', 'NOUN', 'PROPN']:\n",
    "                if word_lower in vocab_dict :\n",
    "                    vocab_dict[word_lower] = vocab_dict[word_lower] + 1\n",
    "                else:\n",
    "                    vocab_dict[word_lower] = 1\n",
    "print(len(vocab_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:39.738823Z",
     "start_time": "2019-09-20T10:20:39.734821Z"
    }
   },
   "outputs": [],
   "source": [
    "# vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram and Trigram list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:40.627798Z",
     "start_time": "2019-09-20T10:20:39.993817Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_list = []\n",
    "for line in training_set[['pos', 'entities']].itertuples():\n",
    "    pos    = line[1];  pos_modified = pos\n",
    "    entities = line[2]\n",
    "    \n",
    "    if entities!=None and type(entities)!=float:\n",
    "        if len(entities.strip()) ==0:\n",
    "            continue\n",
    "        for entity in entities.split('\\t'):\n",
    "            #print(entity)\n",
    "            entity_split = entity.split('/')\n",
    "            entity_text  = entity_split[0]\n",
    "            tag          = entity_split[1]\n",
    "            for entity_text_split in entity_text.split(' '):\n",
    "                if len(entity_text_split.strip())>0:\n",
    "                    pos_modified = pos_modified.replace(entity_text_split, '<' + tag + '>')\n",
    "                    \n",
    "                    break\n",
    "                    \n",
    "    sent_list.append(re.sub(r'\\s+',' ', pos_modified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:41.128794Z",
     "start_time": "2019-09-20T10:20:41.124792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47176"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:41.797780Z",
     "start_time": "2019-09-20T10:20:41.794776Z"
    }
   },
   "outputs": [],
   "source": [
    "# for sentence in sent_list:\n",
    "#     print(sentence)\n",
    "#     print((re.sub(r'\\s+', ' ', getTextFromPOSData(sentence))))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:42.387765Z",
     "start_time": "2019-09-20T10:20:42.374767Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:42.934758Z",
     "start_time": "2019-09-20T10:20:42.930755Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation_list = []\n",
    "for char in string.punctuation:\n",
    "    punctuation_list.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:45.892687Z",
     "start_time": "2019-09-20T10:20:43.547741Z"
    }
   },
   "outputs": [],
   "source": [
    "bigrams_list = []\n",
    "from nltk import ngrams\n",
    "for sentence in sent_list:\n",
    "    clean_sentence = re.sub(r'\\s+', ' ', getTextFromPOSData(sentence))\n",
    "\n",
    "    n = 2\n",
    "    bigrams = ngrams(clean_sentence.split(), n)\n",
    "    for grams in bigrams:\n",
    "        grams = [item.lower() for item in grams]\n",
    "        bigrams_list.append(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:50.546626Z",
     "start_time": "2019-09-20T10:20:47.811651Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_bigram_list = []\n",
    "for grams in bigrams_list:     \n",
    "    for word in grams:\n",
    "        if word in punctuation_list:\n",
    "            remove_bigram_list.append(grams)\n",
    "            break\n",
    "        if word in stop_list:\n",
    "            remove_bigram_list.append(grams)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:53.264533Z",
     "start_time": "2019-09-20T10:20:52.871545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(527161, 2)\n",
      "(50426, 2)\n"
     ]
    }
   ],
   "source": [
    "remove_bigram_df = pd.DataFrame(remove_bigram_list)\n",
    "print(remove_bigram_df.shape)\n",
    "remove_bigram_df = remove_bigram_df.drop_duplicates(subset = [0, 1])\n",
    "print(remove_bigram_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:55.688482Z",
     "start_time": "2019-09-20T10:20:55.681501Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_bigram_array = np.array(remove_bigram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:20:58.333428Z",
     "start_time": "2019-09-20T10:20:58.282429Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_bigram_list = []\n",
    "for item in remove_bigram_array:\n",
    "    remove_bigram_list.append(item[0] + '###' + item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:21:00.036389Z",
     "start_time": "2019-09-20T10:21:00.033389Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:21:02.041346Z",
     "start_time": "2019-09-20T10:21:01.745365Z"
    }
   },
   "outputs": [],
   "source": [
    "updated_bigrams_list = []\n",
    "for item in bigrams_list:\n",
    "    updated_bigrams_list.append(item[0] + '###' + item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:21:04.657323Z",
     "start_time": "2019-09-20T10:21:04.325299Z"
    }
   },
   "outputs": [],
   "source": [
    "count_bigram_dict = {}\n",
    "for item in updated_bigrams_list:\n",
    "    if item not in count_bigram_dict.keys():\n",
    "        count_bigram_dict[item] = 1\n",
    "    else:\n",
    "        count_bigram_dict[item] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:21:06.986243Z",
     "start_time": "2019-09-20T10:21:06.939243Z"
    }
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_count_bigram_list = sorted(count_bigram_dict.items(), key=operator.itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:21:09.318192Z",
     "start_time": "2019-09-20T10:21:09.249192Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "sorted_count_bigram_dict = dict(collections.OrderedDict(sorted_count_bigram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:21:11.716171Z",
     "start_time": "2019-09-20T10:21:11.707139Z"
    }
   },
   "outputs": [],
   "source": [
    "key_list = list(sorted_count_bigram_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:22:02.454061Z",
     "start_time": "2019-09-20T10:21:14.576077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104273\n",
      "53847\n"
     ]
    }
   ],
   "source": [
    "print(len(sorted_count_bigram_dict))\n",
    "for item in key_list:\n",
    "    if item in remove_bigram_list:\n",
    "        del sorted_count_bigram_dict[item]\n",
    "print(len(sorted_count_bigram_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:22:04.927006Z",
     "start_time": "2019-09-20T10:22:04.922008Z"
    }
   },
   "outputs": [],
   "source": [
    "# sorted_count_bigram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:22:07.394962Z",
     "start_time": "2019-09-20T10:22:07.388953Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_list = stopwords.words('english')\n",
    "\n",
    "import string\n",
    "punctuation_list = []\n",
    "for char in string.punctuation:\n",
    "    punctuation_list.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:22:12.181849Z",
     "start_time": "2019-09-20T10:22:09.821903Z"
    }
   },
   "outputs": [],
   "source": [
    "trigrams_list = []\n",
    "from nltk import ngrams\n",
    "for sentence in sent_list:\n",
    "    clean_sentence = re.sub(r'\\s+', ' ', getTextFromPOSData(sentence))\n",
    "\n",
    "    n = 3\n",
    "    trigrams = ngrams(clean_sentence.split(), n)\n",
    "    for grams in trigrams:\n",
    "        grams = [item.lower() for item in grams]\n",
    "        trigrams_list.append(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:22:18.921707Z",
     "start_time": "2019-09-20T10:22:16.058769Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_trigram_list = []\n",
    "for grams in trigrams_list:   \n",
    "\n",
    "    for word in grams:\n",
    "        if word in punctuation_list:\n",
    "            remove_trigram_list.append(grams)\n",
    "            break\n",
    "            \n",
    "        if (word == grams[0] and word in stop_list) or (word == grams[2] and word in stop_list):\n",
    "            remove_trigram_list.append(grams)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:22:24.629588Z",
     "start_time": "2019-09-20T10:22:24.624607Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove_trigram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:22:31.144449Z",
     "start_time": "2019-09-20T10:22:30.538495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480186, 3)\n",
      "(145251, 3)\n"
     ]
    }
   ],
   "source": [
    "remove_trigram_df = pd.DataFrame(remove_trigram_list)\n",
    "print(remove_trigram_df.shape)\n",
    "remove_trigram_df = remove_trigram_df.drop_duplicates(subset = [0, 1, 2])\n",
    "print(remove_trigram_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:22:37.341320Z",
     "start_time": "2019-09-20T10:22:37.323315Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_trigram_array = np.array(remove_trigram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:22:43.922204Z",
     "start_time": "2019-09-20T10:22:43.791181Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_trigram_list = []\n",
    "for item in remove_trigram_array:\n",
    "    remove_trigram_list.append(item[0] + '###' + item[1] + '###' + item[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:22:47.911088Z",
     "start_time": "2019-09-20T10:22:47.908089Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:22:52.602991Z",
     "start_time": "2019-09-20T10:22:52.264001Z"
    }
   },
   "outputs": [],
   "source": [
    "updated_trigrams_list = []\n",
    "for item in trigrams_list:\n",
    "    updated_trigrams_list.append(item[0] + '###' + item[1] + '###' + item[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:22:57.710881Z",
     "start_time": "2019-09-20T10:22:57.209892Z"
    }
   },
   "outputs": [],
   "source": [
    "count_trigram_dict = {}\n",
    "for item in updated_trigrams_list:\n",
    "    if item not in count_trigram_dict.keys():\n",
    "        count_trigram_dict[item] = 1\n",
    "    else:\n",
    "        count_trigram_dict[item] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:23:03.072768Z",
     "start_time": "2019-09-20T10:23:02.999767Z"
    }
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_count_trigram_list = sorted(count_trigram_dict.items(), key=operator.itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:23:08.535688Z",
     "start_time": "2019-09-20T10:23:08.383656Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "sorted_count_trigram_dict = dict(collections.OrderedDict(sorted_count_trigram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:23:14.404530Z",
     "start_time": "2019-09-20T10:23:14.384527Z"
    }
   },
   "outputs": [],
   "source": [
    "key_list = list(sorted_count_trigram_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:28:50.790708Z",
     "start_time": "2019-09-20T10:23:20.070405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241542\n",
      "96291\n"
     ]
    }
   ],
   "source": [
    "print(len(sorted_count_trigram_dict))\n",
    "for item in key_list:\n",
    "    if item in remove_trigram_list:\n",
    "        del sorted_count_trigram_dict[item]\n",
    "print(len(sorted_count_trigram_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:28:57.369592Z",
     "start_time": "2019-09-20T10:28:57.366591Z"
    }
   },
   "outputs": [],
   "source": [
    "# fw = open('trigram.txt', 'w', encoding='utf8')\n",
    "# for word in sorted_count_trigram_dict:\n",
    "#     fw.write(\",\".join(word.split('###')) + \",\"  + str(sorted_count_trigram_dict[word]) + '\\n')\n",
    "# fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:29:03.102788Z",
     "start_time": "2019-09-20T10:29:03.097790Z"
    }
   },
   "outputs": [],
   "source": [
    "# fw = open('bigram.txt', 'w', encoding='utf8')\n",
    "# for word in sorted_count_bigram_dict:\n",
    "#     fw.write(\",\".join(word.split('###')) + \",\"  + str(sorted_count_bigram_dict[word]) + '\\n')\n",
    "# fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:29:08.556725Z",
     "start_time": "2019-09-20T10:29:08.540723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "sorted_x = sorted(vocab_dict.items(), key=operator.itemgetter(1))\n",
    "sorted_x_filtered = [vocab for vocab in sorted_x if vocab[1] >= threshold_for_vocab_selection and vocab not in stop_word_list]\n",
    "print(len(sorted_x_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:29:14.001657Z",
     "start_time": "2019-09-20T10:29:13.996658Z"
    }
   },
   "outputs": [],
   "source": [
    "# ### Saving Vocab file\n",
    "# fw = open('vocab_list_V1.txt', 'w')\n",
    "# for word in sorted_x_filtered:\n",
    "#     fw.write(word[0] + '\\n')\n",
    "# fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:29:19.819587Z",
     "start_time": "2019-09-20T10:29:19.798589Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reading vocab file\n",
    "vocab_list = set([word for word in open('vocab_list_V1.txt', 'r').read().split('\\n') if len(word.strip())>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:29:25.282523Z",
     "start_time": "2019-09-20T10:29:25.277520Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1022, 53847, 96291)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list), len(sorted_count_bigram_dict), len(sorted_count_trigram_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:29:30.719455Z",
     "start_time": "2019-09-20T10:29:30.645455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53847 50957\n"
     ]
    }
   ],
   "source": [
    "bigram_dict = {}\n",
    "for key, value in sorted_count_bigram_dict.items():\n",
    "    bigram_dict['###'.join([item for item in key.split('###') if len(item) > 3])] = value\n",
    "print(len(sorted_count_bigram_dict), len(bigram_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:29:36.939417Z",
     "start_time": "2019-09-20T10:29:36.598386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96291 89204\n"
     ]
    }
   ],
   "source": [
    "trigram_dict = {}\n",
    "for key, value in sorted_count_trigram_dict.items():\n",
    "    trigram_dict['###'.join([item for item in key.split('###') if len(key.split('###')[0]) > 3 and len(key.split('###')[2]) > 3])] = value\n",
    "print(len(sorted_count_trigram_dict), len(trigram_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:29:42.718309Z",
     "start_time": "2019-09-20T10:29:42.708314Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_bigram_dict = {key: value for key, value in bigram_dict.items() if value > 23}\n",
    "filtered_trigram_dict = {key: value for key, value in trigram_dict.items() if value > 23}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:29:48.525241Z",
     "start_time": "2019-09-20T10:29:48.519242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1022, 1484, 1040)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list), len(filtered_bigram_dict), len(filtered_trigram_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:29:54.122178Z",
     "start_time": "2019-09-20T10:29:54.118177Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_vocab_list = list(set(filtered_bigram_dict.keys()))\n",
    "bigram_vocab_list = set([' '.join(item.split('###')) for item in bigram_vocab_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:30:02.238077Z",
     "start_time": "2019-09-20T10:30:02.227079Z"
    }
   },
   "outputs": [],
   "source": [
    "trigram_vocab_list = list(set(filtered_trigram_dict.keys()))\n",
    "trigram_vocab_list = set([' '.join(item.split('###')) for item in trigram_vocab_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T10:30:09.381991Z",
     "start_time": "2019-09-20T10:30:09.376991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1022, 1484, 1040)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list), len(bigram_vocab_list), len(trigram_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T13:26:05.725642Z",
     "start_time": "2019-09-16T13:26:05.717649Z"
    }
   },
   "outputs": [],
   "source": [
    "def generateNGrams(sentence, n):\n",
    "    ngrams_list = []\n",
    "    clean_sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    ngrams_ = ngrams(clean_sentence.split(), n)\n",
    "    for grams in ngrams_:\n",
    "        grams = [item.lower() for item in grams]\n",
    "        ngrams_list.append(' '.join(grams))\n",
    "        \n",
    "    return ngrams_list\n",
    "\n",
    "# generateNGrams('hi i am karAn', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T13:26:26.867778Z",
     "start_time": "2019-09-16T13:26:26.859776Z"
    }
   },
   "outputs": [],
   "source": [
    "def creating_feature_file(file_name, data_set):\n",
    "    fw = open(file_name, 'w', encoding='utf8')\n",
    "    #fw.write('Sentence,')\n",
    "    for word in vocab_list:\n",
    "        fw.write(word + ',')\n",
    "    for word in bigram_vocab_list:\n",
    "        fw.write(word + ',')\n",
    "    for word in trigram_vocab_list:\n",
    "        fw.write(word + ',')\n",
    "    fw.write('Line Item Rate,Timekeeper Level,UTBMS Activity Code,UTBMS Phase Code,UTBMS Task Code,Work Area,Work Area Level 2')\n",
    "    fw.write(',Is_Prior_Approved,Is_Action,Class_Label\\n')\n",
    "\n",
    "    for line in data_set[['pos', 'Line Item Rate','Timekeeper Level','UTBMS Activity Code',\\\n",
    "             'Work Area', 'Work Area Level 2','Is_Prior_Approved', \\\n",
    "             'Is_Action', 'Adjusted', 'UTBMS Phase Code',  'UTBMS Task Code', 'Line Item Description']].itertuples():\n",
    "        sentence_root = [get_rootword(word.split('/')[0], word.split('/')[1]) for word in line[1].split()]\n",
    "        intersection_list = list(set(sentence_root) & vocab_list)\n",
    "        #fw.write(line.sentence_text + ',')\n",
    "        for word in vocab_list:\n",
    "            if word in intersection_list:\n",
    "                fw.write(str(1) + ',')\n",
    "            else:\n",
    "                fw.write(str(0) + ',')\n",
    "                \n",
    "        ## Bigram and Trigram logic\n",
    "        bigram_sentence_root = generateNGrams(line[11], n = 2)\n",
    "        bigram_intersection_list = list(set(bigram_sentence_root) & bigram_vocab_list)\n",
    "        for word in bigram_vocab_list:\n",
    "            if word in bigram_intersection_list:\n",
    "                fw.write(str(1) + ',')\n",
    "            else:\n",
    "                fw.write(str(0) + ',')\n",
    "                \n",
    "        trigram_sentence_root = generateNGrams(line[11], n = 3)\n",
    "        trigram_intersection_list = list(set(trigram_sentence_root) & trigram_vocab_list)\n",
    "        for word in trigram_vocab_list:\n",
    "            if word in trigram_intersection_list:\n",
    "                fw.write(str(1) + ',')\n",
    "            else:\n",
    "                fw.write(str(0) + ',')\n",
    "        \n",
    "        ## Line Item Rate, Timekeeper Level,UTBMS Activity Code'\n",
    "        fw.write(str(line[2]) + \",\" + str(line[3]) + \",\" + str(line[4]))\n",
    "        ## 'UTBMS Phase Code', 'UTBMS Task Code',\n",
    "        fw.write(\",\" + str(line[10]) + \",\" + str(line[11]))\n",
    "        ## 'Work Area', 'Work Area Level 2',\n",
    "        fw.write(\",\" + line[5] + \",\" + line[6])\n",
    "        # 'Is_Prior_Approved','Is_Action'\n",
    "        if line[7]:\n",
    "            fw.write(\",\" + str(1) + \",\" + str(line[8]))\n",
    "        else:\n",
    "            fw.write(\",\" + str(0) + \",\" + str(line[8]))\n",
    "        \n",
    "        ## Adjusted\n",
    "        fw.write(\",\" + str(line[9]) + '\\n')\n",
    "\n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T13:28:08.427845Z",
     "start_time": "2019-09-16T13:26:33.164867Z"
    }
   },
   "outputs": [],
   "source": [
    "creating_feature_file('bag_of_words_training_V1.csv', training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T13:19:47.657330Z",
     "start_time": "2019-09-16T13:19:17.457652Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('bag_of_words_training_V1.csv',\n",
    "                        sep='|',  dtype='unicode', error_bad_lines=False)\n",
    "# except CParserError:\n",
    "#     print(\"Something wrong the file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T13:20:11.759960Z",
     "start_time": "2019-09-16T13:20:11.754924Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
